<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Building an Event Streaming System for Distributed Change Processing</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 960px;
      margin: 40px auto;
      line-height: 1.6;
      padding: 0 20px;
    }
    h1, h2, h3 {
      color: #222;
    }
    code {
      background: #f2f2f2;
      padding: 2px 4px;
      border-radius: 3px;
      font-family: monospace;
    }
    pre {
      background: #f8f8f8;
      padding: 10px;
      overflow-x: auto;
    }
    img {
      max-width: 100%;
      margin: 20px 0;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: left;
    }
    th {
      background: #f0f0f0;
    }
  </style>
</head>
<body>

<h1>Building an Event Streaming System for Distributed Change Processing</h1>

<h2>What Is Event Streaming?</h2>
<p>At its core, an <strong>event</strong> is a discrete record of an update to a single row in a database table. It captures <em>what</em> changed, <em>where</em> it changed, and <em>who</em> made the change. An <strong>event stream</strong> is a continuously flowing, append-only log of these changes — delivered in real time to the services that care about them.</p>

<p>This model inverts the traditional control flow. Instead of services polling the database to ask, “Has anything changed?”, the database itself declares, “Here’s what just happened.” That inversion is powerful — it removes duplication, simplifies logic, and brings clarity to distributed architectures.</p>

<p>Our implementation uses <strong>SQL Server Service Broker</strong> as a reliable messaging mechanism. As updates happen in the transactional database (on a replicated server for performance), each change is wrapped in a small, structured message and pushed onto a queue. That message is then processed by a central handler and appended to the <code>EventStream</code> table.</p>

<p>Every table in the system is supported through automatically generated triggers and code that evolves alongside the schema. Each table sends its changes to a shared processor queue, where the root entities (like <strong>Person</strong>, <strong>Company</strong>, <strong>Assignment</strong>, and <strong>PNB</strong>) are derived and attached to the event.</p>

<p>This approach ensures that:</p>
<ul>
  <li>Every change is captured at the database level — no polling or missed records.</li>
  <li>Each service gets a copy of the message via Service Broker — no duplication of effort.</li>
  <li>Only meaningful events are processed — “just the changes,” as shown below.</li>
</ul>

<img src="assets/just-the-changes.png" alt="Just the changes payload">
<img src="assets/root-entities-summary.png" alt="Root Entities payload">
<img src="assets/root-entities-array.png" alt="Root Entity JSON array">

<p>This gives us a unified, immutable log of all activity across the platform — replacing hundreds of history tables with a single, interpretable event source. The benefits are significant:</p>
<ul>
  <li><strong>Declutters</strong> the database environment by removing table-level history</li>
  <li>Operates on the <strong>replica</strong>, preserving OLTP performance</li>
  <li>Enables <strong>automated debugging</strong> across the platform via queryable logs</li>
  <li><strong>Evolves automatically</strong> with the schema — no manual effort required</li>
  <li>Provides <strong>transparency and traceability</strong> with every step logged</li>
  <li>Empowers each service to respond only to changes that affect them</li>
</ul>

<h2>Why We Needed a Change</h2>
<p>The previous solution relied on maintaining a full clone of our transactional system, duplicating every table into a <code>History</code> schema. Changes were tracked by copying entire rows via SQL Agent jobs and writing bespoke ETL logic to derive meaning. This created:</p>
<ul>
  <li>Complexity and fragility across 591 tables</li>
  <li>Expensive, slow, and redundant batch jobs</li>
  <li>Inconsistent logic across consumers</li>
  <li>Delayed visibility into system state</li>
  <li>A “thundering herd” of consumers querying the same data at once</li>
</ul>

<h2>Design Principles</h2>
<ul>
  <li>Create a single source of truth for all changes</li>
  <li>Push changes to consumers immediately after commit</li>
  <li>Identify root entities (Person, Company, Assignment) affected by a change</li>
  <li>Enable consumers to act on structured, cleanly formatted payloads</li>
  <li>Eliminate polling and redundant logic</li>
</ul>

<h2>Architecture Overview</h2>
<p>The core of the EventStream system is a SQL Server table called <code>EventSource</code>. It acts as an append-only log of all changes in the transactional database.</p>

<h3>Event Flow Diagram</h3>
<img src="assets/event-flow-diagram.png" alt="Event Flow: Example from PersonJob">

<h3>How It Works</h3>
<p>We auto-generate triggers for all transactional tables using PowerShell. Each trigger compares new and old row versions, generates a compact <code>ChangeSet</code> JSON payload, and inserts it into the central <code>EventSource</code> table.</p>

<p>A SQL Server trigger on <code>EventSource</code> then fires, identifies downstream consumers via routing metadata, and enqueues a message into each product’s Service Broker queue.</p>

<p>Message listeners consume these queues, passing the payload into a processor that parses it, traces root entities, and produces a semantically rich change event for the product to act on.</p>

<h3>Resolving Root Entities with Graph Traversal</h3>
<p>One of the biggest challenges in building a generalized event streaming system is understanding the broader context of a change — specifically, which high-level business entities (e.g. Person, Company, Assignment) are impacted by a modification in a deeply normalized table.</p>

<p>A prime example is the table <code>PersonJobCompanyLocationUsage</code>, which tracks the relationship between a person, their job, and the specific office location where that job is held. It sits at the junction of multiple entity hierarchies:</p>
<ul>
  <li><code>Person → PersonJob → PersonJobCompanyLocationUsage</code></li>
  <li><code>Company → CompanyLocation → PersonJobCompanyLocationUsage</code></li>
</ul>

<p>To solve this, I wrote a PowerShell script that introspects the relational schema, walking the foreign key constraints across all tables. This script generates a set of entities and relationships, treating foreign keys as directed edges in a graph. I then loaded this representation into a graph database (Neo4j) and used Cypher queries to discover all paths from a given table to a root entity.</p>

<p>For each table, we could now compute the traversal paths that reliably lead to <code>Person</code>, <code>Company</code>, or <code>Assignment</code>. These traversals were converted into reusable SQL fragments executed at runtime to determine the root entities for any row-level change — powering the <code>AffectedEntities</code> column in the <code>EventSource</code> table.</p>


<img src="assets/resolving-root-entities.png" alt="Resolving Root Entities">

<img src="assets/entity-traversal-diagram.png" alt="Root Entity Traversal Diagram">

<h2>EventSource Table Schema</h2>
<img src="assets/eventsource-schema.png" alt="EventSource Table Schema">

<h2>Downstream Consumers</h2>
<p>The system powers multiple product domains:</p>
<ul>
  <li><strong>AssignmentAudit</strong> – renders readable audit trails</li>
  <li><strong>ConfidentialAccess</strong> – enforces secure access in real time</li>
  <li><strong>Reports</strong> – drives data warehousing and dashboards</li>
  <li><strong>Restrictions</strong> – supports rule-based compliance logic</li>
  <li><strong>SOLR</strong> – incrementally updates search indexes</li>
</ul>

<h2>Operational Impact</h2>
<ul>
  <li>Over 10 million events processed monthly</li>
  <li>Data latency reduced to under a second</li>
  <li>Batch jobs deprecated or simplified</li>
  <li>Data lineage and auditing centralized</li>
</ul>

<h2>What’s Next</h2>
<p>We are continuing to evolve the system internally with plans to:</p>
<ul>
  <li>Abstract logic further into metadata-driven routing</li>
  <li>Expand the event structure to include more semantic typing</li>
  <li>Adopt a version-controlled event replay mechanism</li>
</ul>

<h2>Final Thoughts</h2>
<p>This event streaming system modernized our legacy SQL infrastructure with real-time change awareness, reproducibility, and auditability — without requiring a complete replatforming effort. It remains a foundational part of our architecture, powering everything from audit logs to user permissions to search indexing.</p>

<p>It’s a strong example of how architectural thinking, automation, and metadata design can create lasting, scalable platforms inside even the most traditional relational environments.</p>

<hr>


</body>
</html>
